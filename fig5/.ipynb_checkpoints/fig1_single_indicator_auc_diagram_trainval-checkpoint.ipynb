{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a4dc33-5506-4848-90f9-a289dd113f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stats_fun'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstats_fun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_statistic_info, cal_ci95\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/eye_team/kangdang/mmc_project_2023_submission/exp_records/fusion_exps/random_mi_single_variable/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stats_fun'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as pathlib\n",
    "from ..stats_fun import get_statistic_info, cal_ci95\n",
    "import matplotlib.pyplot as plt\n",
    "base_path = '/mnt/eye_team/kangdang/mmc_project_2023_submission/exp_records/fusion_exps/random_mi_single_variable/'\n",
    "\n",
    "def get_score_and_gt_helper(base_path, method_id, feature_id, fusion_type='metric_only'):\n",
    "    input_dir = pathlib.join(base_path, method_id)\n",
    "    val_pred_list = list()\n",
    "    val_gt_list = list()\n",
    "    test_pred_list = list()\n",
    "    test_gt_list = list()\n",
    "    for mice_id in range(20):\n",
    "        curr_folder = pathlib.join(input_dir, 'mice_imputation_' + str(mice_id+ 1), fusion_type)\n",
    "        train_score = pd.read_csv(pathlib.join(curr_folder, \"feat_[\" + str(feature_id) + \"]_train_pred_result.csv\"))\n",
    "        val_score = pd.read_csv(pathlib.join(curr_folder, \"feat_[\" + str(feature_id) + \"]_val_pred_result.csv\"))\n",
    "        pred = list(train_score['pred'])\n",
    "        pred.extend(list(val_score['pred']))\n",
    "        gt = list(train_score['gt'])\n",
    "        gt.extend(list(val_score['gt']))\n",
    "        pred = np.array(pred)\n",
    "        gt = np.array(gt)\n",
    "        val_pred_list.append(pred)\n",
    "        val_gt_list.append(gt)\n",
    "        test_pred_list.append(pred)\n",
    "        test_gt_list.append(gt)\n",
    "\n",
    "    assert((val_gt_list[0] == val_gt_list[1]).all())\n",
    "    assert((test_gt_list[0] == test_gt_list[1]).all())\n",
    "    val_gt_list = val_gt_list[0]\n",
    "    test_gt_list = test_gt_list[0]\n",
    "    val_pred_list = np.mean(np.array(val_pred_list), axis = 0)\n",
    "    test_pred_list = np.mean(np.array(test_pred_list), axis = 0)\n",
    "    return val_gt_list, test_gt_list, val_pred_list, test_pred_list\n",
    "\n",
    "\n",
    "# image only result\n",
    "test_result_pickle = pickle.load(open('/mnt/eye_team/kangdang/mmc_project_2023_submission/exp_records/image_only_exps/top_checkpoint_ensemble_train_result.pickle', 'rb'))\n",
    "test_pred_set = test_result_pickle['pred_set']\n",
    "test_gt_set = test_result_pickle['gt_set']\n",
    "test_result_pickle = pickle.load(open('/mnt/eye_team/kangdang/mmc_project_2023_submission/exp_records/image_only_exps/top_checkpoint_ensemble_val_result.pickle', 'rb'))\n",
    "test_pred_set = np.concatenate((test_pred_set, test_result_pickle['pred_set']), axis = 0)\n",
    "test_gt_set = np.concatenate((test_gt_set, test_result_pickle['gt_set']), axis = 0)\n",
    "print(test_pred_set.shape)\n",
    "\n",
    "auc95_dl, fpr_dl, tpr_dl, auc_dl = get_statistic_info(test_gt_set, test_pred_set)\n",
    "print(auc95_dl)\n",
    "auc95, rec, sep, sep_90, global_auc, global_rec, global_sep, global_sep_90, _, _ = cal_ci95(test_gt_set, test_pred_set, 0.8)\n",
    "print(rec)\n",
    "print(global_rec)\n",
    "print(sep)\n",
    "print(global_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68609713-5b8d-4215-b8fd-cdb5ecf9a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.metrics as sklm\n",
    "import os.path as pathlib\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "def get_score_and_gt(base_path, feature_id, fusion_type='metric_only',  method_id_list = ['method_lr_basic', 'method_lda_basic','method_gbc_basic', 'method_rf_basic',  'method_lsvm_basic',  'method_rbfsvm_basic']):\n",
    "    val_auc_list = []\n",
    "    for method_id in method_id_list:\n",
    "        val_gt_list, _,val_pred_list, _ = get_score_and_gt_helper(base_path, method_id, feature_id, fusion_type)\n",
    "        fpr, tpr, threshold = metrics.roc_curve(val_gt_list, val_pred_list)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        val_auc_list.append(auc)\n",
    "    \n",
    "    best_val_idx = np.argmax(val_auc_list)\n",
    "    best_method_id = np.array(method_id_list)[best_val_idx]\n",
    "    val_gt_list, test_gt_list, val_pred_list, test_pred_list = get_score_and_gt_helper(base_path, best_method_id, feature_id, fusion_type)\n",
    "    return best_method_id, val_gt_list, test_gt_list, val_pred_list, test_pred_list\n",
    "\n",
    "def get_statistic_info(test_gt_list, test_pred_list):\n",
    "    auc95, rec, sep, sep_90, global_auc, global_rec, global_sep, global_sep_90, _, _ = cal_ci95(test_gt_list, test_pred_list, 0.8)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(test_gt_list, test_pred_list)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return auc95, fpr, tpr, auc\n",
    "\n",
    "def get_indicator_all_results():\n",
    "    indicator_id_list = [\"'1'\", \"'2'\", \"'3'\", \"'4'\", \"'5'\", \"'6'\", \"'7'\", \"'8'\", \"'9'\", \"'10'\", \"'11'\", \"'12'\"]\n",
    "    indicator_name_list = ['Gender', 'Age', 'Diastolic Bp', 'Systolic Bp',  'Heart Rate', 'BMI', 'Course of Diabetes', 'Hypertension', 'Hyperlipidemia', 'Cardiovascular Disease', 'Classification of smoker', 'Classification of drinker']\n",
    "    result_dict = dict()\n",
    "    for indicator_id, indicator_name in zip(indicator_id_list, indicator_name_list):\n",
    "        print(indicator_name)\n",
    "        best_method_id, val_gt_list, test_gt_list, val_pred_list, test_pred_list = get_score_and_gt(base_path, indicator_id, method_id_list = ['method_lr_basic'])\n",
    "        auc95, fpr, tpr, auc = get_statistic_info(test_gt_list, test_pred_list)\n",
    "        result_dict[indicator_name] = [best_method_id, auc95, fpr, tpr, auc]\n",
    "    return result_dict\n",
    "\n",
    "result_dict = get_indicator_all_results()\n",
    "sort_dict = dict()\n",
    "for key, value in result_dict.items():\n",
    "    sort_dict[key] = -value[-1]\n",
    "sort_keys = sorted(sort_dict, key=sort_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42321c9-aa43-4c85-92bc-75df8b5b1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "my_dpi=120\n",
    "plt.figure(figsize=(960/my_dpi, 960/my_dpi), dpi=my_dpi)\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "palette = plt.get_cmap('Set3')\n",
    "cmap = mpl.cm.get_cmap('PuOr')\n",
    "plt.plot(fpr_dl, tpr_dl, marker='', color='red', linewidth=6, alpha=0.7, label='Image only\\nAUC='+ \"{:.3f}\".format(auc_dl)+ '[{:.3f}'.format(auc95_dl[0]) + ',{:.3f}'.format(auc95_dl[1]) + ']')\n",
    "prespace = [12, 3, 0, 3, 3, 0, 0, 3, 6, 3, 6, 0]\n",
    "for idx, key in enumerate(sort_keys):\n",
    "    value = result_dict[key]\n",
    "    fpr = value[2]\n",
    "    tpr = value[3]\n",
    "    label = key + '\\nAUC= ' + \"{:.2f}\".format(value[4]) + '[{:.3f}'.format(value[1][0]) + ',{:.3f}'.format(value[1][1]) + ']'\n",
    "    color = cmap((idx/len(sort_keys)))\n",
    "    plt.plot(fpr,tpr, marker='', color=color, linewidth=3, alpha=0.5, label=label)\n",
    "plt.legend(prop={'size': 14})\n",
    "\n",
    "ax=plt.gca()\n",
    "ax.xaxis.label.set_fontsize(15)\n",
    "ax.yaxis.label.set_fontsize(15)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5),labelspacing=1, prop={'size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da7c8f-89ca-4ab0-98ff-8f410eff24bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
